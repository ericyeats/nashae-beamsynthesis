{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1e3296",
   "metadata": {},
   "source": [
    "# VAE Beamsynthesis\n",
    "### Overview\n",
    "This script is used to run all Beamsynthesis tests involving VAE (BetaVAE, $\\beta=1$), BetaVAE, FactorVAE, or BetaTCVAE. The script will train a VAE-based model on a fixed amount of data using the hyperparameters defined in the cell below. The script will generate latent traversals, compare original data against reconstructions of data, create 3D visualizations of the learned latent space, and evaluate the latent space using the BetaVAE disentanglement metric.\n",
    "\n",
    "### Instructions\n",
    "Set hyperparameters for the run in the cell below. Then, hit Run All on the jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(0, 1000)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "### set hyperparameters for this run\n",
    "\n",
    "from ae_utils_exp import B_TCVAE as VAE_BASED_MODEL # change <model> in \".... import <model> as ....\"\n",
    "### options: VAE (for VAE, BetaVAE), FACTOR_VAE, or B_TCVAE (for BetaTCVAE)\n",
    "\n",
    "beta = 125. # \\beta for BetaVae, FactorVAE, and BetaTCVAE\n",
    "n_lat = 4 # VAE bottleneck size (m)\n",
    "batch_size = 100 # batch size used for training\n",
    "lr = 0.01 # learning rate used for training. should be 1e-4 if the model is FACTOR_VAE\n",
    "\n",
    "print(\"Seed: \", seed)\n",
    "print(\"Batch Size: \", batch_size)\n",
    "print(\"Learning Rate: \", lr)\n",
    "print(\"Beta: \", beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import matplotlib.pyplot as plt\n",
    "from ae_utils_exp import s_init, beam_s2s2_norm, beam_s2s2_inorm\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures import enc_beamform_vae as enc\n",
    "from architectures import dec_beamform as dec\n",
    "\n",
    "inp_bn = beam_s2s2_norm\n",
    "ae = VAE_BASED_MODEL(inp_bn, enc(lat=n_lat), dec(lat=n_lat), device, z_dim=n_lat, inp_inorm=beam_s2s2_inorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00559994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the dataset retrieval\n",
    "# loadfunc: given a \"x.xxx.npy\" file, return a tensor version and its 'name'\n",
    "loadfunc = lambda path: (torch.tensor(np.load(path)).type(torch.float), path[-10:-4], path[-16:-11],)\n",
    "tform = lambda x: x[0]\n",
    "dataset = DatasetFolder(\"./beamsynthesis\", loadfunc, (\".npy\",), transform=Compose([tform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fb753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_loss, kl_loss = \\\n",
    "    ae.fit(dataset, 100, beta=beta, lr=lr,\\\n",
    "           batch_size=batch_size, generator_ae=torch.Generator().manual_seed(0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curves on a log scale\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].set_ylabel(\"$log_{10}$(Reconstruction Loss)\")\n",
    "ax[1].set_ylabel(\"$log_{10}$(KL Loss)\")\n",
    "ax[0].set_xlabel(\"Group\")\n",
    "ax[1].set_xlabel(\"Group\")\n",
    "ax[0].plot(np.log10(rec_loss), linewidth=2, label='Reconstruction')\n",
    "ax[1].plot(np.log10(kl_loss), linewidth=2, label='KL')\n",
    "ax[0].grid(True, which='both', ls='-')\n",
    "ax[1].grid(True, which='both', ls='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ba4bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_scores, std_scores, inp, rec = ae.record_latent_space(dataset, batch_size=10, n_batches=10)\n",
    "\n",
    "z_base = z_scores[1]\n",
    "\n",
    "fig, ax = plt.subplots(ae.z_dim, 5, figsize=(16,16))\n",
    "for i in range(ae.z_dim):\n",
    "    _min = z_scores[:, i].min()\n",
    "    _max = z_scores[:, i].max()\n",
    "    variation = torch.linspace(_min, _max, steps=5)\n",
    "    for j in range(len(variation)):\n",
    "        z = z_base.clone()\n",
    "        z[i] = variation[j]\n",
    "        out = ae.dec(z.to(ae.device)).detach()\n",
    "        if _max - _min > 0.2:\n",
    "            ax[i][j].plot(out.squeeze().cpu().numpy(), linewidth=2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original data vs. reconstructions and associate latent encodings with data generating factor ground truth\n",
    "\n",
    "dataset = DatasetFolder(\"./beamsynthesis\", loadfunc, (\".npy\",))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "ae.eval()\n",
    "\n",
    "n_ex = len(dataloader)\n",
    "latent = np.zeros((n_ex, ae.z_dim))\n",
    "\n",
    "param1 = np.zeros((n_ex,))\n",
    "param2 = np.zeros((n_ex,))\n",
    "use_param2 = False\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "f_ind = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, lab,) in enumerate(dataloader):\n",
    "            ex = data[0].to(device).detach_()\n",
    "            par1 = float(data[1][0])\n",
    "            param1[i] = par1\n",
    "            if (len(data) > 2):\n",
    "                use_param2 = True\n",
    "                par2 = float(data[2][0])\n",
    "                param2[i] = par2 # param2 in this case is S1_duty_cycle\n",
    "            out = ae(ex).squeeze()\n",
    "            latent[i] = ae.mu.cpu().numpy()\n",
    "            ex = inp_bn(ex)\n",
    "            \n",
    "            if (i)%(len(dataloader)//11) == 0 and f_ind < 10:\n",
    "                ind = f_ind//5, f_ind%5\n",
    "                if use_param2:\n",
    "                    axes[ind].set_title(\"DC:{:1.3f}   FR:{:1.3f}\".format(par2, par1))\n",
    "                else:\n",
    "                    axes[ind].set_title(\"Param: {:1.3f}\".format(par1))\n",
    "                axes[ind].plot(ex[0].cpu().numpy(), linewidth=2, label='in')\n",
    "                axes[ind].plot(out.cpu().numpy(), linewidth=2, label='out')\n",
    "                axes[ind].legend()\n",
    "                f_ind += 1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for viewing the latent space in 3D\n",
    "view_alt=5\n",
    "view_ang=90\n",
    "alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learned latent space in 3D (two views)\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = None\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "for i in range(ae.z_dim):\n",
    "    ax.scatter(param2, param1, latent[..., i], label='L{}'.format(i+1), alpha=alpha)\n",
    "\n",
    "ax.view_init(view_alt, view_ang)\n",
    "ax.set_xlabel('S2_duty_cycle')\n",
    "ax.set_ylabel('S2_frequency')\n",
    "ax.set_zlabel('Latent Activation')\n",
    "#ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "for i in range(ae.z_dim):\n",
    "    ax.scatter(param2, param1, latent[..., i], label='L{}'.format(i+1), alpha=alpha)\n",
    "\n",
    "ax.view_init(view_alt, view_ang + 45)\n",
    "ax.set_xlabel('S2_duty_cycle')\n",
    "ax.set_ylabel('S2_frequency')\n",
    "ax.set_zlabel('Latent Activation')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41756b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of learned latent features\n",
    "ave_vars = std_scores.square().mean(dim=0)\n",
    "print(\"BetaVAE Learned Features: \", (ave_vars <= 0.8).sum().item())\n",
    "z_max = z_scores.max(dim=0)[0]\n",
    "z_min = z_scores.min(dim=0)[0]\n",
    "print(\"FactorVAE, BetaTCVAE Learned Features: \", (z_max - z_min >= 2.).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1be8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [10., 15., 20.]\n",
    "# frequency\n",
    "for f in freqs:\n",
    "    filt = param1 == f\n",
    "    _z_scores = torch.tensor(latent[filt, :])\n",
    "    _n_zs = filt.sum()\n",
    "    _z_diff = (_z_scores[:_n_zs//2] - _z_scores[_n_zs//2:]).abs()\n",
    "    print(f)\n",
    "    print(_z_diff.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e661af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the BetaVAE disentanglement metric\n",
    "from ae_utils_exp import DisentanglementMetric as DM\n",
    "dm = DM(n_lat, 2, lr=1.0)\n",
    "freqs = [10., 15., 20.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941683eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the disentanglement metric linear classifier\n",
    "n_iterations = 10000\n",
    "bsize = 20\n",
    "losses = torch.zeros(n_iterations)\n",
    "for i in range(n_iterations):\n",
    "    # construct the batch\n",
    "    batch = torch.zeros((bsize, n_lat))\n",
    "    # randomly choose data generating factor to hold constant\n",
    "    is_freq = torch.rand(1) > 0.5\n",
    "    for b_ind in range(bsize):\n",
    "        if is_freq: # this is ind 1\n",
    "            # randomly choose a frequency\n",
    "            freq_ind = int(torch.rand(1)*3)\n",
    "            filt = param1 == freqs[freq_ind]\n",
    "            \n",
    "        else: # dc is ind 0\n",
    "            # randomly choose a duty cycle\n",
    "            tenths = torch.randint(low=2, high=8, size=(1,)).item()\n",
    "            hundredths = torch.randint(low=0, high=10, size=(1,)).item()\n",
    "            thousandths = 0 if torch.rand(1) > 0.5 else 5\n",
    "            dc = tenths * 100 + hundredths * 10. + thousandths\n",
    "            filt = param2*1000. == dc\n",
    "        _z_scores = torch.tensor(latent[filt, :])\n",
    "        _z_scores = _z_scores[torch.randperm(_z_scores.shape[0])]\n",
    "        # _z_scores is shuffled, select the difference of the first 2 as elem\n",
    "        ex = (_z_scores[0] - _z_scores[1]).abs()\n",
    "        batch[b_ind] = ex\n",
    "    # batch is now constructed\n",
    "    # train on batch\n",
    "    prediction = dm(batch)\n",
    "    loss = dm.fit_batch(1 if is_freq else 0, batch.cpu())\n",
    "    losses[i] = loss\n",
    "plt.figure()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3840fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the disentanglement metric linear classifier\n",
    "n_iterations = 1000\n",
    "bsize = 20\n",
    "n_correct = 0\n",
    "for i in range(n_iterations):\n",
    "    if i == int(n_iterations*0.9):\n",
    "        dm.set_lr(0.001)\n",
    "    # construct the batch\n",
    "    batch = torch.zeros((bsize, n_lat))\n",
    "    # randomly choose data generating factor to hold constant\n",
    "    is_freq = i >= n_iterations//2\n",
    "    for b_ind in range(bsize):\n",
    "        if is_freq: # this is ind 1\n",
    "            # randomly choose a frequency\n",
    "            freq_ind = int(torch.rand(1)*3)\n",
    "            filt = param1 == freqs[freq_ind]\n",
    "        else: # dc is ind 0\n",
    "            # randomly choose a duty cycle\n",
    "            tenths = torch.randint(low=2, high=8, size=(1,)).item()\n",
    "            hundredths = torch.randint(low=0, high=10, size=(1,)).item()\n",
    "            thousandths = 0 if torch.rand(1) > 0.5 else 5\n",
    "            dc = tenths * 100 + hundredths * 10. + thousandths\n",
    "            filt = param2*1000. == dc\n",
    "        _z_scores = torch.tensor(latent[filt, :])\n",
    "        _z_scores = _z_scores[torch.randperm(_z_scores.shape[0])]\n",
    "        # _z_scores is shuffled, select the difference of the first 2 as elem\n",
    "        ex = (_z_scores[0] - _z_scores[1]).abs()\n",
    "        batch[b_ind] = ex\n",
    "    # batch is now constructed\n",
    "    # train on batch\n",
    "    prediction = dm(batch.mean(dim=0).unsqueeze(0))\n",
    "    n_correct += 1. if prediction == is_freq else 0.\n",
    "print(\"Acc: {:1.2f}\".format(n_correct/n_iterations*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed8cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

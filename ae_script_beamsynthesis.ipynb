{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7dca25",
   "metadata": {},
   "source": [
    "# NashAE Beamsynthesis\n",
    "### Overview\n",
    "This script is used to run all Beamsynthesis tests involving NashAE or AE (NashAE, $\\lambda=0$). The script will train a NashAE on a fixed amount of data using the hyperparameters defined in the cell below. The script will generate latent traversals, plot predicted latent variables against true latent variables, compare original data against reconstructions of data, create 3D visualizations of the learned latent space, and evaluate the latent space using the BetaVAE disentanglement metric.\n",
    "\n",
    "### Instructions\n",
    "Set hyperparameters for the run in the cell below. Then, hit Run All on the jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(0, 1000)\n",
    "random.seed(seed)\n",
    "\n",
    "### Set hyperparameters for this run\n",
    "\n",
    "ar = 0.2 # adversarial ratio (\\lambda)\n",
    "\n",
    "n_lat = 4 # size of the AE bottleneck (m)\n",
    "\n",
    "batch_size = 100 # batch size for training (keep below 360)\n",
    "\n",
    "lr = 0.001 # learning rate for training\n",
    "\n",
    "print(\"Seed: \", seed)\n",
    "print(\"Batch Size: \", batch_size)\n",
    "print(\"LR: \", lr)\n",
    "print(\"AdvRatio: \", ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import matplotlib.pyplot as plt\n",
    "from ae_utils_exp import AutoEncoder, beam_s2s2_norm, beam_s2s2_inorm\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures import enc_beamform as enc\n",
    "from architectures import dec_beamform as dec\n",
    "\n",
    "ae = AutoEncoder(beam_s2s2_norm, enc(lat=n_lat), dec(lat=n_lat), device, z_dim=n_lat, inp_inorm=beam_s2s2_inorm, z_act=torch.nn.Sigmoid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00559994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the dataset retrieval\n",
    "# loadfunc: given a \"x.xxx.npy\" file, return a tensor version and its 'name'\n",
    "loadfunc = lambda path: (torch.tensor(np.load(path)).type(torch.float), path[-10:-4], path[-16:-11],)\n",
    "tform = lambda x: x[0]\n",
    "dataset = DatasetFolder(\"./beamsynthesis\", loadfunc, (\".npy\",), transform=Compose([tform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fb753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train NashAE on the Beamsynthesis dataset for 100 groups of batches\n",
    "rec_loss, adv_loss, pred_loss = \\\n",
    "    ae.fit(dataset, 100, ar=ar, preds_train_iters=5, lr=lr,\\\n",
    "           batch_size=batch_size, generator_ae=torch.Generator().manual_seed(0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curves on a log scale\n",
    "plt.figure()\n",
    "plt.ylabel(\"$log_{10}$(AE Loss)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(np.log10(rec_loss), linewidth=2, label='Reconstruction')\n",
    "plt.plot(np.log10(adv_loss.abs()), linewidth=2, label='Adversarial')\n",
    "plt.plot(np.log10(pred_loss), linewidth=2, label='Predictor')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', ls='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ba4bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot traversals of the latent space\n",
    "\n",
    "z_scores, z_pred_scores, inp, rec = ae.record_latent_space(dataset, batch_size=10, n_batches=10)\n",
    "z_base = z_scores[1]\n",
    "\n",
    "fig, ax = plt.subplots(ae.z_dim, 5, figsize=(16,16))\n",
    "for i in range(ae.z_dim):\n",
    "    _min = z_scores[:, i].min()\n",
    "    _max = z_scores[:, i].max()\n",
    "    variation = torch.linspace(_min, _max, steps=5)\n",
    "    for j in range(len(variation)):\n",
    "        z = z_base.clone()\n",
    "        z[i] = variation[j]\n",
    "        out = ae.dec(z.to(ae.device)).detach()\n",
    "        if _max - _min > 0.2:\n",
    "            ax[i][j].plot(out.squeeze().cpu().numpy(), linewidth=2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afe1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ae_utils_exp import covariance\n",
    "\n",
    "fig, axes = plt.subplots(1, ae.z_dim, figsize=(16, 4))\n",
    "for ind in range(ae.z_dim):\n",
    "    axes[ind].tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, left=False, labelleft=False)\n",
    "    axes[ind].scatter(z_scores[..., ind], z_pred_scores[..., ind])\n",
    "    axes[ind].set_xlim((-0.05, 1.05))\n",
    "    axes[ind].set_ylim((-0.05, 1.05))\n",
    "    cov = covariance(z_scores[..., ind], z_pred_scores[..., ind]).item()\n",
    "    std = z_scores[..., ind].std(dim=0).item()\n",
    "    std_p = z_pred_scores[..., ind].std(dim=0).item()\n",
    "    rho2 = 0.\n",
    "    if std > 0. and std_p > 0.:\n",
    "        rho2 = (cov/(std*std_p))**2\n",
    "    axes[ind].set_title(\"R2: {:1.3f}\".format(rho2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original data vs. reconstructions\n",
    "# push all beamsynthesis data through the model to associate all latent encodings with data generating factors\n",
    "\n",
    "dataset = DatasetFolder(\"./beamsynthesis\", loadfunc, (\".npy\",))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "ae.eval()\n",
    "\n",
    "n_ex = len(dataloader)\n",
    "latent = np.zeros((n_ex, ae.z_dim))\n",
    "latent_predict = np.empty((n_ex, ae.z_dim))\n",
    "param1 = np.zeros((n_ex,))\n",
    "param2 = np.zeros((n_ex,))\n",
    "use_param2 = False\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "f_ind = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, lab,) in enumerate(dataloader):\n",
    "            ex = data[0].to(device).detach_()\n",
    "            par1 = float(data[1][0])\n",
    "            param1[i] = par1\n",
    "            if (len(data) > 2):\n",
    "                use_param2 = True\n",
    "                par2 = float(data[2][0])\n",
    "                param2[i] = par2 # param2 in this case is S1_duty_cycle\n",
    "            out = ae(ex).squeeze()\n",
    "            latent[i] = ae.z.cpu().numpy()\n",
    "            latent_predict[i] = ae.z_pred.cpu().numpy()\n",
    "            ex = beam_s2s2_norm(ex)\n",
    "            if (i)%(len(dataloader)//11) == 0 and f_ind < 10:\n",
    "                ind = f_ind//5, f_ind%5\n",
    "                if use_param2:\n",
    "                    axes[ind].set_title(\"DC:{:1.3f}   FR:{:1.3f}\".format(par2, par1))\n",
    "                else:\n",
    "                    axes[ind].set_title(\"Param: {:1.3f}\".format(par1))\n",
    "                axes[ind].plot(ex[0].cpu().numpy(), linewidth=2, label='in')\n",
    "                axes[ind].plot(out.cpu().numpy(), linewidth=2, label='out')\n",
    "                axes[ind].legend()\n",
    "                f_ind += 1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the 3D depictions of the latent space\n",
    "view_alt=5\n",
    "view_ang=90\n",
    "alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeead73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of learned latent features\n",
    "_mins, _min_indices = z_scores.min(dim=0)\n",
    "_maxes, _max_indices = z_scores.max(dim=0)\n",
    "diff = _maxes - _mins\n",
    "print(\"Number of Learned Features: \", (diff > 0.2).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learned latent space\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = None\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "for i in range(ae.z_dim):\n",
    "    ax.scatter(param2, param1, latent[..., i], label='L{}'.format(i+1), alpha=alpha)\n",
    "\n",
    "ax.view_init(view_alt, view_ang)\n",
    "ax.set_xlabel('S2_duty_cycle')\n",
    "ax.set_ylabel('S2_frequency')\n",
    "ax.set_zlabel('Latent Activation')\n",
    "#ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "for i in range(ae.z_dim):\n",
    "    ax.scatter(param2, param1, latent[..., i], label='L{}'.format(i+1), alpha=alpha)\n",
    "\n",
    "ax.view_init(view_alt, view_ang + 45)\n",
    "ax.set_xlabel('S2_duty_cycle')\n",
    "ax.set_ylabel('S2_frequency')\n",
    "ax.set_zlabel('Latent Activation')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted latent space\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = None\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for i in range(ae.z_dim):\n",
    "    ax.scatter(param2, param1, latent_predict[..., i], label='L{}'.format(i+1), alpha=alpha)\n",
    "\n",
    "ax.view_init(view_alt, view_ang)\n",
    "ax.set_xlabel('S2_duty_cycle')\n",
    "ax.set_ylabel('S2_frequency')\n",
    "ax.set_zlabel('Latent Prediction')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1be8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the BetaVAE Disentanglement metric linear classifier\n",
    "from ae_utils_exp import DisentanglementMetric as DM\n",
    "dm = DM(n_lat, 2, lr=1.0)\n",
    "freqs = [10., 15., 20.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941683eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the disentanglement metric linear classifier\n",
    "n_iterations = 10000\n",
    "bsize = 20\n",
    "losses = torch.zeros(n_iterations)\n",
    "for i in range(n_iterations):\n",
    "    # construct the batch\n",
    "    batch = torch.zeros((bsize, n_lat))\n",
    "    # randomly choose data generating factor to hold constant\n",
    "    is_freq = torch.rand(1) > 0.5\n",
    "    for b_ind in range(bsize):\n",
    "        if is_freq: # this is ind 1\n",
    "            # randomly choose a frequency\n",
    "            freq_ind = int(torch.rand(1)*3)\n",
    "            filt = param1 == freqs[freq_ind]\n",
    "            \n",
    "        else: # dc is ind 0\n",
    "            # randomly choose a duty cycle\n",
    "            tenths = torch.randint(low=2, high=8, size=(1,)).item()\n",
    "            hundredths = torch.randint(low=0, high=10, size=(1,)).item()\n",
    "            thousandths = 0 if torch.rand(1) > 0.5 else 5\n",
    "            dc = tenths * 100 + hundredths * 10. + thousandths\n",
    "            filt = param2*1000. == dc\n",
    "        _z_scores = torch.tensor(latent[filt, :])\n",
    "        _z_scores = _z_scores[torch.randperm(_z_scores.shape[0])]\n",
    "        # _z_scores is shuffled, select the difference of the first 2 as elem\n",
    "        ex = (_z_scores[0] - _z_scores[1]).abs()\n",
    "        batch[b_ind] = ex\n",
    "    # batch is now constructed\n",
    "    # train on batch\n",
    "    loss = dm.fit_batch(1 if is_freq else 0, batch.cpu())\n",
    "    losses[i] = loss\n",
    "plt.figure()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb32433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the BetaVAE Disentanglement metric linear classifier\n",
    "n_iterations = 1000\n",
    "bsize = 20\n",
    "n_correct = 0\n",
    "for i in range(n_iterations):\n",
    "    # construct the batch\n",
    "    batch = torch.zeros((bsize, n_lat))\n",
    "    # randomly choose data generating factor to hold constant\n",
    "    is_freq = i >= n_iterations//2\n",
    "    for b_ind in range(bsize):\n",
    "        if is_freq: # this is ind 1\n",
    "            # randomly choose a frequency\n",
    "            freq_ind = int(torch.rand(1)*3)\n",
    "            filt = param1 == freqs[freq_ind]\n",
    "        else: # dc is ind 0\n",
    "            # randomly choose a duty cycle\n",
    "            tenths = torch.randint(low=2, high=8, size=(1,)).item()\n",
    "            hundredths = torch.randint(low=0, high=10, size=(1,)).item()\n",
    "            thousandths = 0 if torch.rand(1) > 0.5 else 5\n",
    "            dc = tenths * 100 + hundredths * 10. + thousandths\n",
    "            filt = param2*1000. == dc\n",
    "        _z_scores = torch.tensor(latent[filt, :])\n",
    "        _z_scores = _z_scores[torch.randperm(_z_scores.shape[0])]\n",
    "        # _z_scores is shuffled, select the difference of the first 2 as elem\n",
    "        ex = (_z_scores[0] - _z_scores[1]).abs()\n",
    "        batch[b_ind] = ex\n",
    "    # batch is now constructed\n",
    "    # train on batch\n",
    "    prediction = dm(batch.mean(dim=0).unsqueeze(0))\n",
    "    n_correct += 1. if prediction == is_freq else 0.\n",
    "print(\"Acc: {:1.2f}\".format(n_correct/n_iterations*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c9214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
